{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Document Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History:\n",
    "\n",
    "- 2017/9/7: \n",
    "    - 移除文章內的google廣告\n",
    "    - 用逗號和問號斷句\n",
    "    - 加入similar words\n",
    "        - check read01_summary\n",
    "    - 加入兩張圖(tf-idf keywords, similar words)\n",
    "        - check keywordmap\n",
    "- 2017/9/11:\n",
    "    - 建立user dict\n",
    "    - 用jieba抓tfidf key words\n",
    "- 2017/9/12:\n",
    "    - word2vec\n",
    "    - pca, tsne for visualization\n",
    "    - LDA\n",
    "- 2017/9/13:\n",
    "    - add tf in bar chart\n",
    "    - bar cahrt for LDA\n",
    "    - w2v k-means clustering \n",
    "    - stop word在jieba.cut沒有作用, 改成再斷詞時另外處理\n",
    "        - check stpwrdlst\n",
    "- 2017/9/15\n",
    "    - 過濾非文章內容的sentence and word\n",
    "        - check data_delete, data_clean\n",
    "    - userdict.txt:\n",
    "        - 工業四點零\n",
    "    - 加入英文stop words\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- 用scikit-learn学习LDA主题模型\n",
    "    - http://www.cnblogs.com/pinard/p/6908150.html\n",
    "    - http://blog.csdn.net/eastmount/article/details/50824215\n",
    "    \n",
    "- 中文文本挖掘预处理流程总结\n",
    "    - http://www.cnblogs.com/pinard/p/6744056.html\n",
    "    \n",
    "- Mining English and Korean text with Python\n",
    "    https://www.lucypark.kr/courses/2015-ba/text-mining.html\n",
    "    \n",
    "- NLTK (POS, chunk, Parser tree)    \n",
    "    - http://aweiho2015.pixnet.net/blog/post/10269587-%5B%E8%AA%8D%E8%AD%98%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%28text-mining%29%5D--%E5%A6%82%E4%BD%95%E8%99%95%E7%90%86%E4%B8%80%E5%8F%A5\n",
    "    \n",
    "- TF-IDF and TextRank\n",
    "    - http://zhicongchen.github.io/2016/11/22/TF-IDF-and-TextRank/\n",
    "    \n",
    "- 中文斷詞：斷句不要悲劇 / Head first Chinese text segmentation    \n",
    "    - https://speakerdeck.com/fukuball/head-first-chinese-text-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/jayhsu/work/github/textmining/dict.txt.big ...\n",
      "Loading model from cache /var/folders/yf/hq7ghg4j3ksb8k34wyh8vk2m0000gn/T/jieba.ub36e993abda9bbf53d1f5b38e3ae9b44.cache\n",
      "Loading model cost 2.556 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import random\n",
    "from flask import Flask, g, render_template, request\n",
    "import logging\n",
    "import html_template as ht\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import collections\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from collections import OrderedDict\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "punct = u'''\\n +-％%:!),.:;?]}¢'\"、。〉》」』】〕〗〞︰|︱︳丨﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘—_…~/#><'''\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "jieba.load_userdict('userdict.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#中文字型\n",
    "fp = matplotlib.font_manager.FontProperties(fname = 'font/NotoSansCJKtc-Regular.otf')\n",
    "matplotlib.font_manager.fontManager.ttffiles.append(fp.get_file())\n",
    "font_entry = matplotlib.font_manager.FontEntry(fp.get_file(), name=fp.get_name(),\n",
    "                                               style=fp.get_style(), variant=fp.get_variant(),\n",
    "                                              weight=fp.get_weight(), stretch=fp.get_stretch(), size=fp.get_size())\n",
    "\n",
    "matplotlib.font_manager.fontManager.ttflist.append(font_entry)\n",
    "plt.rcParams['font.family'] = fp.get_name()\n",
    "\n",
    "stpwrdpath = \"model/stop_words_new.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'r')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()\n",
    "\n",
    "jieba.analyse.set_stop_words(stpwrdpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort2list(list1, list2):\n",
    "    list1, list2 = (list(t) for t in zip(*sorted(zip(list1, list2),reverse=True)))\n",
    "    return list1, list2\n",
    "\n",
    "def keywords(row, voc_dic, kwtopn=3, withWeight=False):\n",
    "    keywork_list=[]\n",
    "    keyworkWweight_list=[]\n",
    "    tfidf_vec = row['tfidf_vector']\n",
    "    topn = list(set(tfidf_vec))\n",
    "    topn.sort(reverse=True)\n",
    "    for top in topn[:kwtopn]:\n",
    "        if top==0:\n",
    "            continue\n",
    "        winner = np.argwhere([x ==top for x in tfidf_vec ])\n",
    "        idxlist =winner.flatten().tolist() \n",
    "        for i in idxlist:\n",
    "            keywork_list.append(voc_dic.get(i))\n",
    "            keyworkWweight_list.append((voc_dic.get(i), top)  )\n",
    "        if len(keywork_list)>=kwtopn:\n",
    "            break\n",
    "    if withWeight:\n",
    "        return tuple(keyworkWweight_list) \n",
    "    else:\n",
    "        return tuple(keywork_list) \n",
    "    \n",
    "def docsummary(row):\n",
    "    doc_list = row['sentences']\n",
    "    keywords = row['keywords']\n",
    "    if keywords:\n",
    "        keysentencelist=[]\n",
    "        for doc in doc_list:\n",
    "            doc = doc.lower()\n",
    "            for kw in keywords:\n",
    "                if kw in doc and doc not in keysentencelist:\n",
    "                    keysentencelist.append(doc)\n",
    "                    \n",
    "        #summary_str = ','.join(keysentencelist)\n",
    "        summary_str=''\n",
    "        strlen = 0\n",
    "        for sen in keysentencelist[:10]:\n",
    "            if strlen>50:\n",
    "                strlen = len(sen)\n",
    "                summary_str=summary_str+',\\n'+sen\n",
    "            else:\n",
    "                strlen = strlen + len(sen)\n",
    "                summary_str=summary_str+','+sen\n",
    "    else:\n",
    "        summary_str='NA'\n",
    "        \n",
    "    return summary_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleanregex = ['\\[SS_\\w+\\]','\\(adsbygoogle.+\\)\\;']\n",
    "def data_clean(dirty_str):\n",
    "    tmp_str = dirty_str\n",
    "    for pat in data_cleanregex:\n",
    "        tmp_str = re.sub(pat, '', tmp_str)\n",
    "    return tmp_str\n",
    "\n",
    "data_deleteregex_ = [r'.*\\.srt$']\n",
    "data_deleteregex = list(map(lambda x: re.compile(x), data_deleteregex_))\n",
    "def data_delete(dirty_str):\n",
    "    tmp_str = dirty_str\n",
    "    for pattern in data_deleteregex:\n",
    "        #pattern = re.compile(pat)  \n",
    "        match1 = pattern.match(dirty_str)\n",
    "        if match1:\n",
    "            return True\n",
    "    return False        \n",
    "    \n",
    "    \n",
    "def read01_summary(folder, topic, kw_topn=3):\n",
    "    files = os.listdir('{f}/{w}'.format(f=folder, w=topic))\n",
    "    docs=[]\n",
    "    fileName=[]\n",
    "    \n",
    "    for fidx, fname in enumerate(files):\n",
    "        if fname.startswith('.') or fname.endswith('.flag') :\n",
    "            continue\n",
    "\n",
    "        file = open('{fd}/{w}/{f}'.format(fd=folder, w=topic, f=fname), 'r') \n",
    "        doc =[] \n",
    "        for line in file.readlines():                          \n",
    "            line = line.strip()\n",
    "            if not len(line):                               \n",
    "                continue\n",
    "            if data_delete(line):\n",
    "                continue\n",
    "                \n",
    "            line = data_clean(line)\n",
    "            \n",
    "            doc.append(line) \n",
    "        if len(doc)==0:\n",
    "            continue\n",
    "        docs.append('\\n'.join(doc))\n",
    "        fileName.append(str(fidx)+'_'+fname)\n",
    "\n",
    "    dl_df = pd.DataFrame({'fname':fileName, 'doc_tmp':docs})\n",
    "    dl_df.sort_values(by='fname')\n",
    "    dl_df['sentences']=dl_df['doc_tmp'].map(lambda x: re.split(\"[\\n\\r，。？]+\", x))\n",
    "    dl_df['doc']=dl_df['sentences'].map(lambda x: '\\n'.join(x))\n",
    "    dl_df['words'] =   dl_df['doc'].map(lambda x: [_.lower() for _ in jieba.cut(x) if _ not in punct and _ not in stpwrdlst]) \n",
    "    dl_df['words_str'] = dl_df['words'].map(lambda x: ' '.join(x))\n",
    "\n",
    "    cv = CountVectorizer(stop_words=stpwrdlst)\n",
    "    vecs1 = cv.fit_transform(dl_df['words_str']).toarray()\n",
    "    \n",
    "    doclen= list(dl_df['words'].map(lambda x:len(x)))\n",
    "    tf_vec = [(a/b).tolist() for a,b in zip(vecs1.tolist(), doclen)]\n",
    "    dl_df['tf_vector'] = tf_vec\n",
    "    fea = cv.get_feature_names()\n",
    "    key = range(len(fea))\n",
    "    voc_dic = dict(zip(key, fea))\n",
    "    vockey_dic = dict(zip(fea, key))\n",
    "    tfidf = TfidfTransformer()\n",
    "    vecs2 = tfidf.fit_transform(vecs1).toarray()\n",
    "    dl_df['tfidf_vector'] = [_ for _ in vecs2]\n",
    "    \n",
    "    dl_df['keywords'] = dl_df.apply(keywords, axis=1, args=(voc_dic,kw_topn,False))\n",
    "    dl_df['keywords'] = dl_df['keywords'].map(list)\n",
    "    dl_df['keywords_w'] = dl_df.apply(keywords, axis=1, args=(voc_dic,kw_topn,True))\n",
    "    \n",
    "    #dl_df['keywords_jb'] = dl_df['doc'].map(lambda x: jieba.analyse.tfidf(x,topK=kw_topn, withWeight=True))\n",
    "    #dl_df['keywords_jb'] = dl_df['keywords_jb'].map(list)\n",
    "    #dl_df['summary'] = dl_df.apply(docsummary, axis=1)\n",
    "    \n",
    "    dl_w2v_model = Word2Vec(dl_df['words'], min_count=1, size=100,iter=50)\n",
    "    \n",
    "    all_keywords = sum(list(dl_df['keywords']),[])\n",
    "    vec = np.array([dl_w2v_model[w] for w in all_keywords if w in dl_w2v_model ])\n",
    "    dl_pca = PCA(20)\n",
    "    dl_pca.fit(vec)\n",
    "    \n",
    "    return dl_df, voc_dic,vockey_dic, dl_w2v_model, dl_pca\n",
    "\n",
    "def keywordmap(lyrics_df, idx, w2v_model,pca,voc_dic, kw_size=30):\n",
    "    figures=[]\n",
    "    all_keywords = sum(list(lyrics_df['keywords']),[])\n",
    "    counter = collections.Counter(all_keywords)\n",
    "    if len(set(all_keywords)) <100:\n",
    "        samples = len(set(all_keywords))\n",
    "    else:\n",
    "        samples = 100\n",
    "    topn_keywords = [w[0] for w in counter.most_common(samples)]\n",
    "    this_keyword = lyrics_df['keywords'][idx]\n",
    "\n",
    "    vec = np.array([w2v_model[w] for w in (topn_keywords+this_keyword)])\n",
    "\n",
    "    vec_pca = pca.transform(vec)\n",
    "    topn_pca = vec_pca[:samples]\n",
    "    this_pca = vec_pca[samples:]\n",
    "\n",
    "    tsne = TSNE(perplexity=10, n_components=2, init='pca', n_iter=10000)\n",
    "    lowdim_embs = tsne.fit_transform(vec_pca)\n",
    "\n",
    "    topn_lowdim_embs = lowdim_embs[:samples]\n",
    "    this_lowdim_embs = lowdim_embs[samples:]\n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    figures.append(fig)\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(topn_lowdim_embs[:, 0], topn_lowdim_embs[:, 1], s= 20, label='top 100 words')\n",
    "    plt.scatter(this_lowdim_embs[:, 0], this_lowdim_embs[:, 1], s= 50, label='this topic key words', color='orange')\n",
    "    plt.legend()\n",
    "    plt.title('PCA + T-SNE')\n",
    "    for i, label in enumerate(topn_keywords):\n",
    "        if label in this_keyword:\n",
    "            continue\n",
    "        x, y = topn_lowdim_embs[i][:2]\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=12,\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    for i, label in enumerate(this_keyword):\n",
    "        x, y = this_lowdim_embs[i][:2]\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=14,\n",
    "                     ha='right',\n",
    "                     va='bottom',color='orange')\n",
    "    \n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.scatter(topn_pca[:, 0], topn_pca[:, 1], s= 20, label='top 100 words')\n",
    "    plt.scatter(this_pca[:, 0], this_pca[:, 1], s= 50, label='this topic key words', color='orange')\n",
    "    plt.legend()\n",
    "    plt.title('PCA')\n",
    "    for i, label in enumerate(topn_keywords):\n",
    "        if label in this_keyword:\n",
    "            continue\n",
    "        x, y = topn_pca[i][:2]\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=12,\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    for i, label in enumerate(this_keyword):\n",
    "        x, y = this_pca[i][:2]\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=14,\n",
    "                     ha='right',\n",
    "                     va='bottom',color='orange')\n",
    "\n",
    "\n",
    "    fig2 = plt.figure(figsize=(15,6))\n",
    "    figures.append(fig2)\n",
    "    tfidf_vec = lyrics_df['tfidf_vector'][idx]\n",
    "    topn_vec , topn_word= sort2list(list(tfidf_vec) , list(voc_dic.values()))\n",
    "    plt.bar(range(len(topn_vec[:kw_size])), topn_vec[:kw_size])\n",
    "    plt.xticks(range(len(topn_vec[:kw_size])), topn_word[:kw_size],rotation=60, fontsize=12)\n",
    "    plt.title('tfidf keywords')\n",
    "    #print(topn_word[:n])\n",
    "    #plt.show()\n",
    "    \n",
    "    for kw in lyrics_df['keywords'][idx]:\n",
    "        fig3 = plt.figure(figsize=(15,6))\n",
    "        figures.append(fig3)\n",
    "        similarwords = w2v_model.most_similar(kw, topn=kw_size)\n",
    "        words, similarity = zip(*similarwords)\n",
    "        barlist = plt.bar(range(len(similarity)), similarity)\n",
    "        plt.xticks(range(len(similarity)), words,rotation=60, fontsize=12)\n",
    "        plt.title('similar word of {kw}'.format(kw=kw))\n",
    "        #barlist[0].set_color('r')\n",
    "        #print(words)\n",
    "        #plt.show()\n",
    "    return figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_doc = 'text/techdoc'  \n",
    "topic = '深度學習'#'深度學習' # 'MARK' #資料夾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 一篇文章分析 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dl_df, dl_voc_dic, dl_vockey_dic, dl_w2v_model, dl_pca = read01_summary(folder = folder_doc, topic=topic, kw_topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. 檢查斷詞效果 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docid=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_df['doc_tmp'][docid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('斷詞:')\n",
    "print(', '.join(list(dl_df['words'][docid])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 把需要修正的詞加入自訂義字典 \n",
    " - 紀錄load_userdict後仍然斷錯的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. extract key words **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - TF-IDF (IDF: 100篇read01的深度學習文章)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords_predefined = ['NVIDIA', '黃仁勛', 'GPU', '深度學習']\n",
    "\n",
    "#keywords_predefined = '好不好','把它','重點是','我們要','你要','你去','策略','目的','function','負責','風險','注意','生意','生意模式','marketing','經營','價值','目標','target','我要','給我','我們','可以','你現在','我就可以','營收','計畫','投資','錢','賣錢','賺錢','有錢','數字','百','億','找一個','那個','所以','所以呢','經管','政府','smart','smart office','infocus','夏普','sharp','專注','核心競爭力','董事長','terry','CEO','老大','富士康','AI','Deep learning','IoT','工業4.0','要不要','有沒有','可不可以','媽的','管你','禮拜幾','要要','然後','KPI','某些人名','schedule','追','大概','結論','大概就是這樣'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('keywords:')\n",
    "#print(', '.join(list(v['keywords'][docid])))\n",
    "kw_x, kw_y = zip(*(dl_df['keywords_w'][docid][:40]))\n",
    "kw_y = list(map(lambda x: float('%.3f'% x),kw_y))\n",
    "\n",
    "\n",
    "tfvec=dl_df['tf_vector'][docid]\n",
    "tfidf_df = pd.DataFrame({'keyword':kw_x, 'tfidf':kw_y})\n",
    "tfidf_df['tf']=tfidf_df['keyword'].map(lambda x: tfvec[dl_vockey_dic[x]])\n",
    "display(tfidf_df[:10])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "barlist = plt.bar(range(len(kw_y)), kw_y, label='tfidf')\n",
    "plt.bar(range(len(kw_y)), tfidf_df['tf'], label='tf')\n",
    "plt.xticks(range(len(kw_x)), kw_x,rotation=60, fontsize=12)\n",
    "plt.legend()\n",
    "plt.title('tfidf keywords')\n",
    "\n",
    "for idx, kw in enumerate(kw_x):\n",
    "    if kw.upper() in keywords_predefined:\n",
    "        barlist[idx].set_color('r')\n",
    "#barlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - TF-IDF (IDF: jieba內建語料庫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('keywords_jb:')\n",
    "kw_x, kw_y = zip(*jieba.analyse.tfidf(dl_df['doc'][docid],topK=100, withWeight=True))\n",
    "kw_y = list(map(lambda x: float('%.3f'% x),kw_y))\n",
    "\n",
    "tfvec=dl_df['tf_vector'][docid]\n",
    "tfidf_df = pd.DataFrame({'keyword':kw_x, 'tfidf':kw_y})\n",
    "tfidf_df['tf']=tfidf_df['keyword'].map(lambda x: tfvec[dl_vockey_dic[x.lower()]])\n",
    "display(tfidf_df[:6])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "xtmp=range(len(kw_y[:40]))\n",
    "barlist = plt.bar(xtmp, kw_y[:40], label='tfidf')\n",
    "plt.bar(xtmp, tfidf_df['tf'][:40], label='tf')\n",
    "plt.xticks(xtmp, kw_x[:40],rotation=60, fontsize=12)\n",
    "plt.legend()\n",
    "plt.title('tfidf keywords')\n",
    "plt.ylim(0,0.4)\n",
    "\n",
    "\n",
    "for idx, kw in enumerate(kw_x[:40]):\n",
    "    if kw.upper() in keywords_predefined:\n",
    "        barlist[idx].set_color('r')\n",
    "#barlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Topic **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - word2vec word similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl_w2v_model.most_similar('武漢',topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters=7\n",
    "\n",
    "kw_x = jieba.analyse.tfidf(dl_df['doc'][docid],topK=100, withWeight=False)\n",
    "#kw_x = jieba.analyse.textrank(dl_df['doc'][docid],topK=100, withWeight=False)\n",
    "vec_kw = [list(dl_w2v_model[w.lower()]) for w in kw_x]\n",
    "#vec_kw = np.array ([list(dl_w2v_model[w.lower()]) for w in kw_x])\n",
    "w2vkmeans = KMeans(n_clusters=clusters, random_state=0).fit(np.array(vec_kw))\n",
    "group=w2vkmeans.predict(vec_kw)\n",
    "group_dic ={} \n",
    "for i in range(clusters):\n",
    "    idx = np.argwhere([x ==i for x in group ])\n",
    "    group_dic[i]=np.array(kw_x)[idx.flatten().tolist()].tolist()\n",
    "    \n",
    "display(pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in group_dic.items() ])))\n",
    "\n",
    "\n",
    "vec_pca = dl_pca.transform(vec_kw)\n",
    "tsne = TSNE(perplexity=10, n_components=2, init='pca', n_iter=10000,)\n",
    "vec_tsne = tsne.fit_transform(vec_pca)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "marker = itertools.cycle(['h', 'x', 'o', '*','v','d','h']) \n",
    "for i in range(clusters):\n",
    "    idx = np.argwhere([x ==i for x in group ])\n",
    "    data = vec_pca[idx.flatten().tolist()]\n",
    "    plt.scatter(data[:, 0], data[:, 1], s= 50, label='cluster {g}'.format(g=i),marker=next(marker))\n",
    "plt.legend()\n",
    "#plt.scatter(vec_pca[:, 0], vec_pca[:, 1], s= 20, label='top 100 words')\n",
    "\n",
    "for i, label in enumerate(kw_x):\n",
    "    x, y = vec_pca[i][:2]\n",
    "    if label in keywords_predefined:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color='black'\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=13,\n",
    "                 color=color,\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "for i1, label_1 in enumerate(kw_x):\n",
    "    x1, y1 = vec_pca[i1][:2]\n",
    "    for i2, label_2 in enumerate(kw_x):\n",
    "        x2, y2 = vec_pca[i2][:2]\n",
    "        similarity = dl_w2v_model.similarity(label_1.lower(), label_2.lower())\n",
    "        if(similarity>0.9):\n",
    "            plt.plot([x1,x2], [y1,y2])\n",
    "plt.title('pca')\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(16,16))\n",
    "marker = itertools.cycle(['h', 'x', 'o', '*','v','d','.']) \n",
    "for i in range(clusters):\n",
    "    idx = np.argwhere([x ==i for x in group ])\n",
    "    data = vec_tsne[idx.flatten().tolist()]\n",
    "    plt.scatter(data[:, 0], data[:, 1], s= 50, label='cluster {g}'.format(g=i),marker=next(marker))\n",
    "plt.legend()\n",
    "\n",
    "#plt.scatter(vec_tsne[:, 0], vec_tsne[:, 1], s= 20, label='top 100 words')\n",
    "\n",
    "for i, label in enumerate(kw_x):\n",
    "    x, y = vec_tsne[i][:2]\n",
    "    if label in keywords_predefined:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color='black'\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=13,\n",
    "                 color=color,\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "for i1, label_1 in enumerate(kw_x):\n",
    "    x1, y1 = vec_tsne[i1][:2]\n",
    "    for i2, label_2 in enumerate(kw_x):\n",
    "        x2, y2 = vec_tsne[i2][:2]\n",
    "        similarity = dl_w2v_model.similarity(label_1.lower(), label_2.lower())\n",
    "        if(similarity>0.9):\n",
    "            plt.plot([x1,x2], [y1,y2])\n",
    "plt.title('pca+tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_num = 5\n",
    "doc = dl_df['doc_tmp'][13]\n",
    "paragraph = doc.split('\\n')\n",
    "\n",
    "paragraph_cutstrlist=[]\n",
    "for p in paragraph:\n",
    "    if p =='...':\n",
    "        continue\n",
    "    p_cut = jieba.cut(p)\n",
    "    p_cut_str = ' '.join(p_cut)\n",
    "    paragraph_cutstrlist.append(p_cut_str)\n",
    "    \n",
    "\n",
    "cntVector = CountVectorizer(stop_words=stpwrdlst)\n",
    "cntTf = cntVector.fit_transform(paragraph_cutstrlist)\n",
    "fea = cntVector.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_topics=topic_num,\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "docres = lda.fit_transform(cntTf)\n",
    "\n",
    "topiclist = np.argmax(docres, axis=1)\n",
    "doc_topic = pd.DataFrame({ 'sentence:':paragraph_cutstrlist,'topic':topiclist})\n",
    "display(doc_topic.sort_values(by='topic'))\n",
    "\n",
    "\n",
    "lda_kwlist= []\n",
    "for i in lda.components_:\n",
    "    kw, weight = sort2list(i, fea)\n",
    "    lda_kwlist.append(list(zip(kw,weight)))\n",
    "topic_kw = pd.DataFrame(lda_kwlist)\n",
    "topic_kw = topic_kw.transpose()\n",
    "display(topic_kw[:6])\n",
    "\n",
    "\n",
    "for i in range(topic_num):\n",
    "    kw_y, kw_x,  = zip(*topic_kw[i])\n",
    "    kw_y = list(map(lambda x: float('%.3f'% x),kw_y))\n",
    "\n",
    "    plt.figure(figsize=(13,6))\n",
    "    xtmp=range(len(kw_y[:40]))\n",
    "    barlist = plt.bar(xtmp, kw_y[:40], label='tfidf')\n",
    "    plt.bar(xtmp, tfidf_df['tf'][:40], label='tf')\n",
    "    plt.xticks(xtmp, kw_x[:40],rotation=60, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.title('topic {n}'.format(n=i))\n",
    "    #plt.ylim(0,0.4)\n",
    "\n",
    "    for idx, kw in enumerate(kw_x[:40]):\n",
    "        if kw.upper() in keywords_predefined:\n",
    "            barlist[idx].set_color('r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "** flask web **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "urls = [\n",
    "    ['', '', '/', '']\n",
    "    , ['deeplearningSummary', 'docid,topic', '/deeplearningSummary/0/4', 'test']\n",
    "]\n",
    "df_url = pd.DataFrame(urls, columns=['page', 'ps', 'instance', 'remark']).set_index('page')\n",
    "df_url['instance'] = df_url['instance'].map(lambda x: '<a href=\"%s\">%s</a>'%(x, x) if x != '' else '')\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def get_ps(page, path):\n",
    "    names = df_url.ix[page]['ps'].split(',')\n",
    "    try:\n",
    "        ps = {k: v for k, v in zip(names, path.split('/'))}\n",
    "    except:\n",
    "        ps = {}\n",
    "    return ps\n",
    "\n",
    "def get_route(df_url, page):\n",
    "    return '/%s'%page + ''.join(['/<%s>'%_ for _ in df_url.ix[page]['ps'].split(',') if len(_)>0 ])\n",
    "\n",
    "@app.route('/')\n",
    "def root(**ps):    \n",
    "    cards = [['URL', [['table', df_url.reset_index()[['page', 'instance', 'ps', 'remark']]]] ]]\n",
    "    try:\n",
    "        texts = ['hi, %s'%flask_login.current_user.id]\n",
    "    except:\n",
    "        texts = []\n",
    "    return ht.HtmlTemplate('Root', cards=cards, texts=texts).to_html()\n",
    "\n",
    "\n",
    "topic_list=[w for w in os.listdir('text_dl') if not w.startswith('.')]\n",
    "topic=-1\n",
    "dl_df=None\n",
    "dl_w2v_model=None\n",
    "dl_pca=None\n",
    "dl_voc_dic=None\n",
    "@app.route('/deeplearningSummary/<path:path>')\n",
    "def deeplearningSummary(path=''):\n",
    "    global topic\n",
    "    global dl_df\n",
    "    global dl_w2v_model\n",
    "    global dl_pca\n",
    "    global dl_voc_dic\n",
    "    #print(path)\n",
    "    ps={}\n",
    "    ps = get_ps('deeplearningSummary', path)\n",
    "    #print(ps)\n",
    "    docid = int(ps.get('docid'))  #docid\n",
    "    topic_new = int(ps.get('topic'))  #topic\n",
    "    print(topic_new)\n",
    "    topic_str = topic_list[topic_new]\n",
    "    \n",
    "    if topic!=topic_new:\n",
    "        dl_df, dl_voc_dic, dl_w2v_model, dl_pca = read01_summary(folder = 'text/techdoc', topic=topic_str, kw_topn=50)\n",
    "        tipic=topic_new\n",
    "    \n",
    "    selected_doc = dl_df.ix[docid]\n",
    "    \n",
    "    cards = []\n",
    "    content=[]\n",
    "    \n",
    "    display_detailcols=['fname', 'doc', 'keywords', 'summary']\n",
    "    for col in display_detailcols:\n",
    "        content.append(['text', '<font color=\"blue\">{col}</font>'.format(col=col)])\n",
    "        content.append(['text',selected_doc[col]])\n",
    "    \n",
    "    similarwords=OrderedDict()\n",
    "    for kw in list(dl_df['keywords'][0]):\n",
    "        similarword=dl_w2v_model.most_similar(kw,topn=20)\n",
    "        similarwords[kw]=similarword\n",
    "    similarwords_df = pd.DataFrame(similarwords)\n",
    "    content.append(['text', '<font color=\"blue\">{col}</font>'.format(col='similar words')])\n",
    "    content.append(['table',similarwords_df])\n",
    "    \n",
    "    content.append(['text', '<font color=\"blue\"> Text Summary</font>'])\n",
    "    content.append(['table', dl_df[['fname','keywords','summary']]])\n",
    "    \n",
    "    cards.append(['text summary', content ])\n",
    "    \n",
    "    figs = keywordmap(dl_df, docid, dl_w2v_model, dl_pca,dl_voc_dic, kw_size=50)\n",
    "    \n",
    "    content=[]\n",
    "    for f in figs:\n",
    "        content.append(['fig', f])\n",
    "    cards.append(['keyword map', content ])\n",
    "\n",
    "    \n",
    "    controls = []\n",
    "    controls.append( ht.Control_Select(id='docid', text='doc', options=list(range(dl_df.shape[0])), value=docid, labels=list((dl_df['fname'])) ))\n",
    "    controls.append( ht.Control_Select(id='topic', text='topic', options=list(range(len(topic_list))), value=topic_new, labels=topic_list ))\n",
    "    \n",
    "    return ht.HtmlTemplate('Text Summary App', ps, cards, controls=controls, page='deeplearningSummary').to_html()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0',port=5001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "** 下面都沒用 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
